{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'm going to build a simple language model that will predict the upcoming word using n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "UNK = w2i[\"<unk>\"]\n",
    "\n",
    "def read_dataset(filename):\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            yield [w2i[x] for x in line.strip().split(\" \")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(read_dataset(\"../data/ptb/train.txt\"))\n",
    "\n",
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/ptb/valid.txt\"))\n",
    "i2w = {v:k for k,v in w2i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9999\n"
     ]
    }
   ],
   "source": [
    "nwords = len(w2i)\n",
    "print(nwords)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NGram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModel(nn.Module):\n",
    "    def __init__(self, N_grams, vocab_size, embedding_dim, device):\n",
    "        super(NGramLanguageModel, self).__init__()\n",
    "        self.N_grams = N_grams\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.device = device\n",
    "\n",
    "        self.word_embeddings = nn.ModuleList([nn.Embedding(vocab_size, embedding_dim, device=self.device) for _ in range(N_grams)])\n",
    "        self.bias = nn.Parameter(torch.zeros((1, embedding_dim), device=self.device), requires_grad=True)\n",
    "    \n",
    "    def forward(self, words):\n",
    "        out_emb_list = [self.word_embeddings[i](words[i]) for i in range(self.N_grams)] # List of N_grams elements, each element is of size embedding_dim\n",
    "        out_emb = torch.stack(out_emb_list)\n",
    "        n_gram_emb = out_emb.sum(dim=0, keepdims=True)\n",
    "        out = n_gram_emb + self.bias\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams\n",
    "N = 2\n",
    "embedding_dim = 10\n",
    "\n",
    "model = NGramLanguageModel(N_grams=N, vocab_size=len(w2i), embedding_dim=embedding_dim, device=device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.1)\n",
    "loss_criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and eval [WIP]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnnlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
