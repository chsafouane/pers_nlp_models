{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7e36d3-914e-43ff-8925-788557933cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70678839-24ac-42fa-a622-895e5dccd4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "899626ed-b4e0-4ca3-8afa-2d4fcfca1df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_file = \"../data/parallel/train.ja\"\n",
    "train_trg_file = \"../data/parallel/train.en\"\n",
    "\n",
    "dev_src_file = \"../data/parallel/dev.ja\"\n",
    "dev_trg_file = \"../data/parallel/dev.en\"\n",
    "\n",
    "test_src_file = \"../data/parallel/test.ja\"\n",
    "test_trg_file = \"../data/parallel/test.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30b4f298-e43b-49ef-a01c-500ebcf887d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i_src = defaultdict(lambda: len(w2i_src))\n",
    "w2i_trg = defaultdict(lambda: len(w2i_trg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c6fcb2c-8087-4323-a32f-ea2498338215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(source_filename, target_filename):\n",
    "    with open(source_filename, \"r\") as f_source, open(target_filename, \"r\") as f_target:\n",
    "        for line_src, line_target in zip(f_source, f_target):\n",
    "            sent_src = [w2i_src[x] for x in line_src.strip().split() + ['</s>']]\n",
    "            sent_trg = [w2i_trg[x] for x in ['<s>'] + line_target.strip().split() + ['</s>']]\n",
    "            yield sent_src, sent_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5536fb49-8c4e-4110-baec-d517b5760c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train = list(read(train_src_file, train_trg_file))\n",
    "unk_src = w2i_src[\"<unk>\"]\n",
    "eos_src = w2i_src[\"</s>\"]\n",
    "w2i_src = defaultdict(lambda: unk_src, w2i_src)\n",
    "\n",
    "# Target\n",
    "unk_trg = w2i_trg[\"<unk>\"]\n",
    "eos_trg = w2i_trg[\"</s>\"]\n",
    "sos_trg = w2i_trg[\"<s>\"]\n",
    "w2i_trg = defaultdict(lambda: unk_trg, w2i_trg)\n",
    "i2w_trg = {v: k for k, v in w2i_trg.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58d74681-5d05-43cf-9139-34e72c06e03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nwords_src = len(w2i_src)\n",
    "nwords_trg = len(w2i_trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7aeb7e0-5adc-4b9d-a9f9-e75a2bf2f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = list(read(dev_src_file, dev_trg_file))\n",
    "test = list(read(test_src_file, test_trg_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2514c64-5fd9-440c-8192-ce6d9e6463e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 64\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f794c9e-999f-4ffd-b80f-128a5e94abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Especially in early training, the model can generate basically infinitly without generating an EOS\n",
    "#have a max sent size that you end at\n",
    "MAX_SENT_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf732924-96db-4d67-85b1-f4a3cd292191",
   "metadata": {},
   "source": [
    "# Encoder-Decoder LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f4a6dac-87c7-4140-ad48-3fc91d78f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ENC_DEC(nn.Module):\n",
    "    def __init__(self, nwords_source, nwords_target, embedding_size_enc, embedding_size_dec, hidden_size_encoder, hidden_size_decoder):\n",
    "        super(ENC_DEC, self).__init__()\n",
    "        self.emb_encoder = nn.Embedding(num_embeddings=nwords_source, embedding_dim=embedding_size_enc)\n",
    "        self.emb_decoder = nn.Embedding(num_embeddings=nwords_target, embedding_dim=embedding_size_dec)\n",
    "        self.lstm_encoder = nn.LSTM(input_size=embedding_size_enc, hidden_size=hidden_size_encoder, batch_first=True)\n",
    "        self.lstm_decoder = nn.LSTM(input_size=embedding_size_dec, hidden_size=hidden_size_decoder, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=hidden_size_decoder, out_features=nwords_target)\n",
    "    \n",
    "    def forward(self, input_source, input_target):\n",
    "        # input_source: batch_size, input_length\n",
    "        input_embedding = self.emb_encoder(input_source) # Size: batch_size, input_length, embedding_size_enc\n",
    "        # output_encoder => Size: batch_size, input_length, hidden_size_encoder\n",
    "        output_encoder, (h_n_encoder, c_n_encoder) = self.lstm_encoder(input_embedding)\n",
    "        print(output_encoder.shape)\n",
    "        print(h_n_encoder.shape)\n",
    "        print(c_n_encoder.shape)\n",
    "        \n",
    "        \n",
    "        output_embedding = self.emb_decoder(input_target) # Size: batch_size, output_length, embedding_size_dec\n",
    "        # output_decoder => size: batch_size, output_length, hidden_size_dec\n",
    "        output_decoder, (h_n_decoder, c_n_decoder) = self.lstm_decoder(output_embedding) \n",
    "        print(output_decoder.shape)\n",
    "        print(h_n_decoder.shape)\n",
    "        print(c_n_decoder.shape)\n",
    "        \n",
    "        logits = self.linear(output_decoder) # Size: batch_size, output_length, nwords_target\n",
    "        print(logits.shape)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bb5c2bba-9d61-4b95-b1a6-213b771afc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ENC_DEC(\n",
    "            nwords_source=nwords_src, \n",
    "            nwords_target=nwords_trg, \n",
    "            embedding_size_enc=EMBED_SIZE, \n",
    "            embedding_size_dec=EMBED_SIZE, \n",
    "            hidden_size_encoder=HIDDEN_SIZE, \n",
    "            hidden_size_decoder=HIDDEN_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "08d61b42-c6cb-4215-95cb-d6c52dcadc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([8, 128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([1, 128])\n",
      "torch.Size([8, 7043])\n"
     ]
    }
   ],
   "source": [
    "for sent_input, sent_target in train:\n",
    "    sent_input = torch.tensor(sent_input)\n",
    "    sent_target = torch.tensor(sent_target)\n",
    "    model(sent_input, sent_target)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
