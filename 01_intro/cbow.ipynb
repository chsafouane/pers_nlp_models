{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724dbd9c-e275-44db-b732-e08c70e46598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fed9ef-39f6-4e79-a3f4-2dea8c822866",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2cbb8c-70bb-4c82-a45f-40d4e2e68973",
   "metadata": {},
   "source": [
    "# CBOW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1cb3d38-e177-4339-80e6-e21ab0b8bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBoW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_classes):\n",
    "        super(CBoW, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # One can use `EmbeddingBag` directly instead of `Embedding`\n",
    "        # and summing over the embeddings of the words\n",
    "        # Unfortunatly, `EmbeddingBag` is still not available for mps\n",
    "        self.embedding = nn.Embedding(\n",
    "                                vocab_size,\n",
    "                                embedding_dim,\n",
    "                                device=device,\n",
    "                            )\n",
    "        self.linear = nn.Linear(embedding_dim, n_classes, device=device)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.xavier_uniform_(self.linear.weight)\n",
    "    \n",
    "    def forward(self, words):\n",
    "        out = self.embedding(words) # size: len(words) * embedding_dim\n",
    "        out = out.sum(dim=0, keepdims=True) # size : embedding_dim\n",
    "        out = self.linear(out) # size: n_classes\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87c1a5-9413-4e45-b0e3-b585fdcb5c04",
   "metadata": {},
   "source": [
    "# Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92942776-f4ba-47ab-87e6-3781aed1e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475d59c9-0302-4ed0-9dd8-aa55a03bf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                line = f.readline().lower().strip().split(\" ||| \")\n",
    "                text_class, text = line[0], line[1]\n",
    "                yield ([w2i[word] for word in text.split(\" \")], t2i[text_class])\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792e196b-33ee-48aa-b359-7dcb33f052af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(read_dataset(\"../data/classes/train.txt\"))\n",
    "vocab_size = len(w2i)\n",
    "n_classes = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc5b1c2-95df-4631-a752-f814cb91276e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11402"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ecfc60-f3f7-4d53-b87d-d40192f3bf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10acb59-09c4-42ff-baf9-030887a3889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/classes/dev.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed51871c-6bed-43c8-92fc-95977cd14091",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = CBoW(vocab_size, 128, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7026ebf4-d3d2-4a67-abcd-df6877fee696",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(cbow_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5430cc2d-7060-4267-bf9f-f63ca0423ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Train loss: 1.2553884235493253\n",
      "Iteration 0 - Test accuracy: 0.3527272727272727\n",
      "Iteration 1 - Train loss: 0.5974311805731348\n",
      "Iteration 1 - Test accuracy: 0.33636363636363636\n",
      "Iteration 2 - Train loss: 0.24939595125197025\n",
      "Iteration 2 - Test accuracy: 0.3327272727272727\n",
      "Iteration 3 - Train loss: 0.11656908424438608\n",
      "Iteration 3 - Test accuracy: 0.30727272727272725\n",
      "Iteration 4 - Train loss: 0.052619257893008684\n",
      "Iteration 4 - Test accuracy: 0.3090909090909091\n",
      "Iteration 5 - Train loss: 0.021776127792922744\n",
      "Iteration 5 - Test accuracy: 0.2927272727272727\n",
      "Iteration 6 - Train loss: 0.016617859124244374\n",
      "Iteration 6 - Test accuracy: 0.28545454545454546\n",
      "Iteration 7 - Train loss: 0.007515847627143289\n",
      "Iteration 7 - Test accuracy: 0.30363636363636365\n",
      "Iteration 8 - Train loss: 0.004991540859701035\n",
      "Iteration 8 - Test accuracy: 0.3054545454545455\n",
      "Iteration 9 - Train loss: 0.0023530923137057586\n",
      "Iteration 9 - Test accuracy: 0.3109090909090909\n"
     ]
    }
   ],
   "source": [
    "# Just 10 epochs as the goal is not to train a real model\n",
    "# but just to see if the implementation is working\n",
    "for i in range(10):\n",
    "    train_loss = 0\n",
    "    test_accuracy = 0\n",
    "    for words, sentence_class in train:\n",
    "        words = torch.tensor(words, device=device)\n",
    "        sentence_class = torch.tensor([sentence_class], device=device)\n",
    "        predictions = cbow_model(words)\n",
    "        loss = loss_criterion(predictions, sentence_class)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Iteration {i} - Train loss: {train_loss/len(train)}\")\n",
    "    with torch.no_grad():\n",
    "        for words, sentence_class in dev:\n",
    "            words = torch.tensor(words, device=device)\n",
    "            predictions = cbow_model(words)\n",
    "            predicted_class = np.argmax(predictions.detach().cpu().numpy())\n",
    "            if predicted_class == sentence_class:\n",
    "                test_accuracy += 1\n",
    "    print(f\"Iteration {i} - Test accuracy: {test_accuracy/len(dev)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
