{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "724dbd9c-e275-44db-b732-e08c70e46598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68fed9ef-39f6-4e79-a3f4-2dea8c822866",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16550221-737c-4691-8452-ff6c1a1f3758",
   "metadata": {},
   "source": [
    "# Deep CBoW Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f91b7475-fb71-4b46-9419-d9be7b69568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepCBoW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_classes, hidden_layers_sizes, device):\n",
    "        super(DeepCBoW, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.hidden_layers_sizes = hidden_layers_sizes\n",
    "        self.n_hidden_layers = len(hidden_layers_sizes)\n",
    "        self.device = device\n",
    "        \n",
    "        # Layers ---------------------------------------------------------------------\n",
    "        # Could use EmbeddingBag but not available for mps for the time-being\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_dim, device=self.device)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        \n",
    "        # I could have used LazyLinear to not specify the input size\n",
    "        # but then I wouldn't be able to initialize the weights using\n",
    "        # xavier_uniform_\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(embedding_dim if i==0 else self.hidden_layers_sizes[i-1], self.hidden_layers_sizes[i], device=self.device) \n",
    "            for i in range(self.n_hidden_layers)\n",
    "        ])\n",
    "        \n",
    "        for i in range(self.n_hidden_layers):\n",
    "            nn.init.xavier_uniform_(self.linears[i].weight)\n",
    "        \n",
    "        self.output_layer = nn.Linear(self.hidden_layers_sizes[-1], self.n_classes, device=self.device)\n",
    "    \n",
    "    \n",
    "    def forward(self, words):\n",
    "        emb = self.embedding(words) # size: num_words * embedding_dim\n",
    "        out = emb.sum(dim=0, keepdims=True) # size: 1 * embedding_dim\n",
    "        for layer in self.linears:\n",
    "            out = layer(out)\n",
    "            out = torch.tanh(out)\n",
    "        # size : 1 * last hidden layer size\n",
    "        out = self.output_layer(out) # size: 1 * n_classes\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de87c1a5-9413-4e45-b0e3-b585fdcb5c04",
   "metadata": {},
   "source": [
    "# Applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92942776-f4ba-47ab-87e6-3781aed1e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: len(w2i))\n",
    "t2i = defaultdict(lambda: len(t2i))\n",
    "UNK = w2i[\"<unk>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475d59c9-0302-4ed0-9dd8-aa55a03bf483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path: str):\n",
    "    with open(path, \"r\") as f:\n",
    "        try:\n",
    "            for line in f:\n",
    "                line = f.readline().lower().strip().split(\" ||| \")\n",
    "                text_class, text = line[0], line[1]\n",
    "                yield ([w2i[word] for word in text.split(\" \")], t2i[text_class])\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "792e196b-33ee-48aa-b359-7dcb33f052af",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(read_dataset(\"../data/classes/train.txt\"))\n",
    "vocab_size = len(w2i)\n",
    "n_classes = len(t2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc5b1c2-95df-4631-a752-f814cb91276e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11402"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26ecfc60-f3f7-4d53-b87d-d40192f3bf70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e10acb59-09c4-42ff-baf9-030887a3889c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2i = defaultdict(lambda: UNK, w2i)\n",
    "dev = list(read_dataset(\"../data/classes/dev.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed51871c-6bed-43c8-92fc-95977cd14091",
   "metadata": {},
   "outputs": [],
   "source": [
    "dcbow_model = DeepCBoW(vocab_size, 64, n_classes, [32, 16], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7026ebf4-d3d2-4a67-abcd-df6877fee696",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(dcbow_model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5430cc2d-7060-4267-bf9f-f63ca0423ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Train loss: 1.2302827141230248\n",
      "Iteration 0 - Test accuracy: 0.24\n",
      "Iteration 1 - Train loss: 0.81398574184724\n",
      "Iteration 1 - Test accuracy: 0.28545454545454546\n",
      "Iteration 2 - Train loss: 0.37563385079136946\n",
      "Iteration 2 - Test accuracy: 0.3090909090909091\n",
      "Iteration 3 - Train loss: 0.18602406783273603\n",
      "Iteration 3 - Test accuracy: 0.3018181818181818\n",
      "Iteration 4 - Train loss: 0.09241373560736689\n",
      "Iteration 4 - Test accuracy: 0.29454545454545455\n",
      "Iteration 5 - Train loss: 0.054121699393465277\n",
      "Iteration 5 - Test accuracy: 0.29454545454545455\n",
      "Iteration 6 - Train loss: 0.037876522440588874\n",
      "Iteration 6 - Test accuracy: 0.3145454545454546\n",
      "Iteration 7 - Train loss: 0.019210582926925204\n",
      "Iteration 7 - Test accuracy: 0.25272727272727274\n",
      "Iteration 8 - Train loss: 0.01291785676604353\n",
      "Iteration 8 - Test accuracy: 0.29818181818181816\n",
      "Iteration 9 - Train loss: 0.007584351753250936\n",
      "Iteration 9 - Test accuracy: 0.2672727272727273\n"
     ]
    }
   ],
   "source": [
    "# Just 10 epochs as the goal is not to train a real model\n",
    "# but just to see if the implementation is working\n",
    "for i in range(10):\n",
    "    train_loss = 0\n",
    "    test_accuracy = 0\n",
    "    for words, sentence_class in train:\n",
    "        words = torch.tensor(words, device=device)\n",
    "        sentence_class = torch.tensor([sentence_class], device=device)\n",
    "        predictions = dcbow_model(words)\n",
    "        loss = loss_criterion(predictions, sentence_class)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Iteration {i} - Train loss: {train_loss/len(train)}\")\n",
    "    with torch.no_grad():\n",
    "        for words, sentence_class in dev:\n",
    "            words = torch.tensor(words, device=device)\n",
    "            predictions = dcbow_model(words)\n",
    "            predicted_class = np.argmax(predictions.detach().cpu().numpy())\n",
    "            if predicted_class == sentence_class:\n",
    "                test_accuracy += 1\n",
    "    print(f\"Iteration {i} - Test accuracy: {test_accuracy/len(dev)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
